import streamlit as st
import os
import tempfile
import warnings
import logging

# T·∫Øt warnings kh√¥ng c·∫ßn thi·∫øt
warnings.filterwarnings('ignore')
logging.getLogger('streamlit').setLevel(logging.ERROR)
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_text_splitters import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import shutil

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="RAG Chatbot",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS t√πy ch·ªânh
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
    }
    .stChatMessage {
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
    }
    .upload-section {
        background-color: #f0f2f6;
        padding: 1.5rem;
        border-radius: 10px;
        margin-bottom: 1rem;
    }
    .info-box {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 5px;
        border-left: 4px solid #2196F3;
    }
</style>
""", unsafe_allow_html=True)


class RAGChatbotApp:
    def __init__(self):
        self.vectorstore = None
        self.rag_chain = None
        self.embeddings = None
        self.llm = None
        
    def load_pdfs(self, uploaded_files):
        """Load v√† x·ª≠ l√Ω nhi·ªÅu file PDF"""
        all_texts = []
        
        with st.spinner("üîÑ ƒêang x·ª≠ l√Ω c√°c file PDF..."):
            for uploaded_file in uploaded_files:
                # L∆∞u file t·∫°m
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_path = tmp_file.name
                
                try:
                    # Load PDF
                    loader = PyPDFLoader(tmp_path)
                    documents = loader.load()
                    
                    # Th√™m metadata v·ªÅ t√™n file
                    for doc in documents:
                        doc.metadata['source_file'] = uploaded_file.name
                    
                    # Chia nh·ªè vƒÉn b·∫£n
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=500,
                        chunk_overlap=50,
                        length_function=len
                    )
                    texts = text_splitter.split_documents(documents)
                    all_texts.extend(texts)
                    
                    st.success(f"‚úÖ {uploaded_file.name}: {len(texts)} chunks")
                    
                finally:
                    # X√≥a file t·∫°m
                    os.unlink(tmp_path)
        
        return all_texts
    
    def create_vectorstore(self, texts):
        """T·∫°o ho·∫∑c c·∫≠p nh·∫≠t vector store"""
        with st.spinner("üß† ƒêang t·∫°o embeddings..."):
            if self.embeddings is None:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="keepitreal/vietnamese-sbert",
                    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
                )
            
            # X√≥a vector store c≈© n·∫øu c√≥
            if os.path.exists("./chroma_db"):
                shutil.rmtree("./chroma_db")
            
            # T·∫°o vector store m·ªõi
            self.vectorstore = Chroma.from_documents(
                documents=texts,
                embedding=self.embeddings,
                persist_directory="./chroma_db"
            )
            self.vectorstore.persist()
            
        st.success("‚úÖ Vector store ƒë√£ ƒë∆∞·ª£c t·∫°o!")
    
    @st.cache_resource
    def load_llm(_self, model_name):
        """Load m√¥ h√¨nh LLM (cache ƒë·ªÉ kh√¥ng load l·∫°i)"""
        with st.spinner(f"ü§ñ ƒêang t·∫£i model {model_name}..."):
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto",
                low_cpu_mem_usage=True
            )
            
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.95,
                repetition_penalty=1.15
            )
            
            llm = HuggingFacePipeline(pipeline=pipe)
            
        st.success("‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i!")
        return llm
    
    def format_docs(self, docs):
        """Format documents th√†nh string"""
        return "\n\n".join(doc.page_content for doc in docs)
    
    def setup_rag_chain(self, llm):
        """Thi·∫øt l·∫≠p RAG chain v·ªõi LCEL (LangChain Expression Language)"""
        
        # Template cho prompt
        template = """S·ª≠ d·ª•ng th√¥ng tin sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y n√≥i "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu".

Ng·ªØ c·∫£nh: {context}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi chi ti·∫øt:"""
        
        prompt = PromptTemplate.from_template(template)
        
        # T·∫°o retriever
        retriever = self.vectorstore.as_retriever(search_kwargs={"k": 3})
        
        # T·∫°o RAG chain v·ªõi LCEL
        self.rag_chain = (
            {
                "context": retriever | self.format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # L∆∞u retriever ƒë·ªÉ l·∫•y source documents
        self.retriever = retriever
    
    def ask(self, question):
        """ƒê·∫∑t c√¢u h·ªèi"""
        if self.rag_chain is None:
            return None
        
        # L·∫•y c√¢u tr·∫£ l·ªùi
        answer = self.rag_chain.invoke(question)
        
        # L·∫•y source documents
        source_docs = self.retriever.invoke(question)
        
        return {
            "result": answer,
            "source_documents": source_docs
        }


def main():
    # Header
    st.markdown('<div class="main-header">ü§ñ RAG Chatbot - H·ªèi ƒë√°p t·ª´ PDF</div>', unsafe_allow_html=True)
    
    # Kh·ªüi t·∫°o session state
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = RAGChatbotApp()
    
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    if 'processed_files' not in st.session_state:
        st.session_state.processed_files = []
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C·∫•u h√¨nh")
        
        # Ch·ªçn model
        model_options = {
            "VinALlama 7B (Ti·∫øng Vi·ªát)": "vilm/vinallama-7b-chat",
            "Vistral 7B (Ti·∫øng Vi·ªát)": "Viet-Mistral/Vistral-7B-Chat",
            "Mistral 7B (ƒêa ng√¥n ng·ªØ)": "mistralai/Mistral-7B-Instruct-v0.2",
            "TinyLlama 1B (Nh·∫π)": "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        }
        
        selected_model_name = st.selectbox(
            "Ch·ªçn m√¥ h√¨nh LLM",
            options=list(model_options.keys()),
            help="Ch·ªçn model ph√π h·ª£p v·ªõi c·∫•u h√¨nh m√°y c·ªßa b·∫°n"
        )
        selected_model = model_options[selected_model_name]
        
        st.divider()
        
        # Upload PDFs
        st.header("üìÅ T·∫£i l√™n t√†i li·ªáu")
        uploaded_files = st.file_uploader(
            "Ch·ªçn file PDF",
            type=['pdf'],
            accept_multiple_files=True,
            help="B·∫°n c√≥ th·ªÉ t·∫£i l√™n nhi·ªÅu file PDF c√πng l√∫c"
        )
        
        if uploaded_files:
            st.write(f"**ƒê√£ ch·ªçn {len(uploaded_files)} file:**")
            for file in uploaded_files:
                st.write(f"- {file.name}")
        
        # N√∫t x·ª≠ l√Ω
        if st.button("üöÄ X·ª≠ l√Ω t√†i li·ªáu", type="primary", use_container_width=True):
            if not uploaded_files:
                st.error("‚ö†Ô∏è Vui l√≤ng t·∫£i l√™n √≠t nh·∫•t 1 file PDF")
            else:
                try:
                    # Load PDFs
                    texts = st.session_state.chatbot.load_pdfs(uploaded_files)
                    
                    # T·∫°o vector store
                    st.session_state.chatbot.create_vectorstore(texts)
                    
                    # Load LLM
                    if st.session_state.chatbot.llm is None:
                        st.session_state.chatbot.llm = st.session_state.chatbot.load_llm(selected_model)
                    
                    # Setup RAG chain
                    st.session_state.chatbot.setup_rag_chain(st.session_state.chatbot.llm)
                    
                    # L∆∞u danh s√°ch file ƒë√£ x·ª≠ l√Ω
                    st.session_state.processed_files = [f.name for f in uploaded_files]
                    
                    st.success("üéâ S·∫µn s√†ng tr·∫£ l·ªùi c√¢u h·ªèi!")
                    
                except Exception as e:
                    st.error(f"‚ùå L·ªói: {str(e)}")
        
        st.divider()
        
        # Hi·ªÉn th·ªã th√¥ng tin
        st.header("‚ÑπÔ∏è Th√¥ng tin")
        device = "GPU (CUDA)" if torch.cuda.is_available() else "CPU"
        st.info(f"**Device:** {device}")
        
        if st.session_state.processed_files:
            st.success(f"**ƒê√£ x·ª≠ l√Ω:** {len(st.session_state.processed_files)} file")
            with st.expander("Xem danh s√°ch file"):
                for fname in st.session_state.processed_files:
                    st.write(f"‚úì {fname}")
        
        # N√∫t x√≥a l·ªãch s·ª≠
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    
    # Main chat area
    if not st.session_state.processed_files:
        st.markdown("""
        <div class="info-box">
        <h3>üëã Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi RAG Chatbot!</h3>
        <p>ƒê·ªÉ b·∫Øt ƒë·∫ßu:</p>
        <ol>
            <li>T·∫£i l√™n file PDF ·ªü sidebar b√™n tr√°i</li>
            <li>Ch·ªçn m√¥ h√¨nh LLM ph√π h·ª£p</li>
            <li>Nh·∫•n "X·ª≠ l√Ω t√†i li·ªáu"</li>
            <li>B·∫Øt ƒë·∫ßu ƒë·∫∑t c√¢u h·ªèi!</li>
        </ol>
        </div>
        """, unsafe_allow_html=True)
    else:
        # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                if "sources" in message and message["sources"]:
                    with st.expander("üìÑ Xem ngu·ªìn tr√≠ch d·∫´n"):
                        for i, source in enumerate(message["sources"], 1):
                            st.write(f"**Ngu·ªìn {i}:** {source['file']} (Trang {source['page']})")
                            st.caption(f"_{source['content'][:200]}..._")
        
        # Input chat
        if prompt := st.chat_input("ƒê·∫∑t c√¢u h·ªèi v·ªÅ t√†i li·ªáu..."):
            # Th√™m c√¢u h·ªèi v√†o l·ªãch s·ª≠
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # L·∫•y c√¢u tr·∫£ l·ªùi
            with st.chat_message("assistant"):
                with st.spinner("ü§î ƒêang suy nghƒ©..."):
                    try:
                        result = st.session_state.chatbot.ask(prompt)
                        
                        if result:
                            answer = result["result"]
                            st.markdown(answer)
                            
                            # L∆∞u ngu·ªìn
                            sources = []
                            if result["source_documents"]:
                                for doc in result["source_documents"]:
                                    sources.append({
                                        "file": doc.metadata.get('source_file', 'Unknown'),
                                        "page": doc.metadata.get('page', 'N/A'),
                                        "content": doc.page_content
                                    })
                                
                                with st.expander("üìÑ Xem ngu·ªìn tr√≠ch d·∫´n"):
                                    for i, source in enumerate(sources, 1):
                                        st.write(f"**Ngu·ªìn {i}:** {source['file']} (Trang {source['page']})")
                                        st.caption(f"_{source['content'][:200]}..._")
                            
                            # Th√™m c√¢u tr·∫£ l·ªùi v√†o l·ªãch s·ª≠
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": answer,
                                "sources": sources
                            })
                        else:
                            st.error("Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i.")
                            
                    except Exception as e:
                        st.error(f"‚ùå L·ªói: {str(e)}")


if __name__ == "__main__":
    main()